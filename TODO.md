# TODO

### General
- [X] Split test and train execution into command arguments
- [X] Create cached weights, kernels, biases, etc.
- [X] Create clear cache command
- [X] Create no-cache command
- [X] Create plot command
- [X] Set dropout rate to multiple layers
- [X] Update readme file
- [X] Fix loss and accuracy plot
- [X] Add multiple optimizer
- [X] improve training options
- [X] add learning rate scheduler

### Dense Network
- [X] Create loss and accuracy for each activation defined by the user
- [X] Add dynamic derivative in backpropagation for errors other than softmax
- [X] Update dense trainer

### Conv Network
- [X] Fix max-pooling ``_unpooling`` method (original height and width are incorrect)
- [X] Add kernel bias to forward and backpropagation
- [X] Add different pooling options
- [X] refact class porperties

