# TODO

### General
- [X] Split test and train execution into command arguments
- [X] Create cached weights, kernels, biases, etc.
- [X] Create clear cache command
- [X] Create no-cache command
- [ ] Set dropout rate to multiple layers
- [ ] Update readme file
- [ ] Add multiple optimizer
- [ ] improve training options

### Dense Network
- [X] Create loss and accuracy for each activation defined by the user
- [ ] Add dynamic derivative in backpropagation for errors other than softmax

### Conv Network
- [X] Fix max-pooling ``_unpooling`` method (original height and width are incorrect)
- [X] Add kernel bias to forward and backpropagation
- [ ] Add different pooling options
- [ ] refact class porperties

